---
title: "Time Series Decomposition"
subtitle: ""
author:
  - name: James Livsey
    affiliations:
      - name: U.S. Census Bureau
        address: james.a.livsey@census.gov
  - name: Anna Smyk
    affiliations:
      - name: Insee, France
        address: anna.smyk@insee.fr 
format: revealjs
editor: source
toc: true
toc-depth: 1
slide-number: true
smaller: false
scrollable: true 
execute:
  echo: true
editor_options: 
  chunk_output_type: console
---



```{r, echo=FALSE}
library(forecast)
library(fpp3)
library(tidyverse)
library(tsbox)
library(zoo)
library(seasonal)
library(astsa)
```

# ---- Transformations and Adjustments

## Types of We consider

-   Calendar adjustments
-   Population adjustments
-   Inflation adjustments
-   Mathematical transformations

## Calendar Adjustment

Some of the variation seen in seasonal data may be due to simple calendar effects.

<img src="Jan-2024.jpeg" style="width:500px;"/> <img src="Jan-2027.jpeg" style="width:500px;"/>

## Trading Day Adjustment

```{r}
m = seas(AirPassengers, regression.variables = "td")
summary(m)
```

## Population Adjustments

Any data that are affected by population changes can be adjusted to give per-capita data.

```{r}
global_economy |>
  filter(Country == "Turkey") |>
  mutate(gdp_per_capita = GDP/Population) |>
  ggplot(aes(x = Year, y = gdp_per_capita)) + 
  geom_line() + 
  labs(title= "GDP per capita", ylab = "$US")
```

## Inflation Adjustment

-   Data which are affected by the value of money are best adjusted before modelling.

-   For example, the average cost of a new house will have increased over the last few decades due to inflation. A \$200,000 house this year is not the same as a \$200,000 house twenty years ago.

-   For this reason, financial time series are usually adjusted so that all values are stated in dollar values from a particular year.

    -   For example, the house price data may be stated in year 2000 dollars.

## Inflation Adjustment

-   To make these adjustments, a price index is used.
-   If $z_t$ denotes the price index and $y_t$ denotes the original house price in year $t$
-   Then $x_t = \frac{y_t}{z_t} \times z_{2000}$ gives the adjusted house price at year 2000 dollar values.
-   Price indexes are often constructed by government agencies.
    -   For consumer goods, a common price index is the Consumer Price Index (or CPI).

## Inflation Adjustment Example

```{r}
#| output-location: slide
print_retail <- aus_retail |>
  filter(Industry == "Newspaper and book retailing") |>
  group_by(Industry) |>
  index_by(Year = year(Month)) |>
  summarise(Turnover = sum(Turnover))

aus_economy <- global_economy |>
  filter(Code == "AUS")

print_retail |>
  left_join(aus_economy, by = "Year") |>
  mutate(Adjusted_turnover = Turnover / CPI * 100) |>
  pivot_longer(c(Turnover, Adjusted_turnover),
               values_to = "Turnover") |>
  mutate(name = factor(name,
         levels=c("Turnover","Adjusted_turnover"))) |>
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry",
       y = "$AU")
```

## Mathematical transformations

-   The most common mathematical transfomation is `log`
-   A few versions of logarithmic transformation are given here

```{r, eval=FALSE, echo=TRUE}
log(x)
log1p(x)
log10(x)
sign(x) * log(abs(x))
```

## 

:::: {.columns}

::: {.column width="50%"}

```{r}
plot(AirPassengers)
```

:::

::: {.column width="50%"}

```{r}
plot(log(AirPassengers))
```

:::

::::

# ---- Patterns

## Decomposition

-   Time series data can exhibit a variety of patterns, and it is often helpful to split a time series into several components, each representing an underlying pattern category.
-   When decomposing a time series, it is sometimes helpful to first transform or adjust the series in order to make the decomposition (and later analysis) as simple as possible. So we will begin by discussing transformations and adjustments.

## Trend

A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, when it might go from an increasing trend to a decreasing trend. There is a trend in the antidiabetic drug sales data shown in Figure 2.2.

## Extracting Trend

-   There are many ways to extract trends
-   We will cover they theory more in later lectures

```{r}
trend = ts_trend(AirPassengers)
ts_plot(ts_c(trend, AirPassengers))
```

## Extracting Trend

-   For example, this trend looks too smooth!

```{r}
trend = ts_trend(unemp)
ts_dygraphs(ts_c(trend, unemp))
```

## Seasonal

A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period. The monthly sales of antidiabetic drugs (Figure 2.2) shows seasonality which is induced partly by the change in the cost of the drugs at the end of the calendar year.

## Cyclic

A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the “business cycle”. The duration of these fluctuations is usually at least 2 years.

## Seasonal Plots

```{r}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Plot of Airpassengers series"
#|   - "Plot Broken up by year"

plot(AirPassengers)
as_tsibble(AirPassengers) |> gg_season()
```

## Monthplot

```{r}
ggmonthplot(diff(AirPassengers))
```

## Lag plots

Consider `aus_production` dataset

```{r}
aus_production |>
  filter(year(Quarter) >= 2000) |>
  mutate(quarter_int = quarter(Quarter) |> as.factor()) |>
  ggplot(aes(x = Quarter, y = Beer)) + 
  geom_line() + 
  geom_point(aes(color = quarter_int) )
```

-   A lag plot is a graph showing $y_t$ plotted against $y_{t-k}$ for different values of $k$.

## Lag plots

```{r}
#| output-location: slide
recent_production <- aus_production |>
  filter(year(Quarter) >= 2000)
recent_production |>
  gg_lag(Beer, geom = "point") +
  labs(x = "lag(Beer, k)")
```

# ---- Measures of Dependence

## Autocovariance {.smaller}

Let $\{X_t\}_{t=1}^T$ be a sample from a univariate regularly spaced time series. Statistical inference about the time series requires understanding the joint probability distribution of the process $\{X_t\}_{t = -\infty}^{\infty}$. Knowledge about the distribution of the entire process must be gleaned from that of the observed random variables:

$$ F(x_1, \ldots, x_T) = P(X_1 \leq x_1, \cdots, X_T \leq x_T). $$

Unfortunately, in almost all cases, we have only one sample point (sample path) to work with. Often, to simplify the goal, the objective is reduced to understanding the first and second-order properties of the process:

The mean function defined as $\mu(t) = E(X_t)$ and covariance between the random variables at two time points, referred to as the autocovariance.

$$ C(s; t) = \text{cov}(X_t,X_s), \quad s,t = 0, \pm 1, \pm 2, \ldots $$


## Autocovariance 

Recall the standard correlation coefficient between two random variables $X$ and 
$Y$

```{r, echo=FALSE}
set.seed(1234)
X = rnorm(n = 10, mean = 20, sd = 4)
Y = X + rnorm(n = 10, mean = 0, sd = 2)
```


```{r}
plot(X, Y, pch = 19)
cor(X, Y)
```

## Autocovariance

For a time series, we can calculate the correlation between $X_t$ and $X_{t-1}$ and get the **lag-1 autocorrelation**

```{r}
length(AirPassengers)
xt_1 = AirPassengers[1:143]
xt   = AirPassengers[2:144]
plot(xt_1, xt)
cor(xt_1, xt)
```


## Autocovariance


The autocovariance function is defined as the second moment product

$$\gamma_x(s,t) = \text{cov}(x_s,x_t) = E[(x_s - \mu_s)(x_t - \mu_t)]$$

When no possible confusion exists about which time series we are referring to, we will drop the subscript and write $\gamma_x(s,t)$ as $\gamma(s,t)$.

## Autocovariance of White Noise

The white noise series $w_t$ has $E(w_t) = 0$ and $$ 
\gamma_w(s,t) 
=
\text{cov}(w_s, w_t) 
= 
\begin{cases}
\sigma_w^2 & s = t \\
0 & s \neq t
\end{cases}
$$

# ---- Stationary Time Series

## Weakly Stationary or Second Order Stationary {.smaller}

We will restrict ourselves to the less stringent notion of weakly stationary processes. When the process is a **Gaussian Process**, then weak stationarity implies strong stationarity.

From the definition, it implies that for any $h \in \mathbb{Z}$

$$
\text{cov}(X_t, X_{t+h}) = C(0, h) = \gamma(h).
$$

This is generally called the **autocovariance function (ACF)** for the stationary process $X_t$.

Understanding properties of this function $\gamma : \mathbb{Z} \to \mathbb{R}$ is key to understanding properties of the stationary time series and being able to predict its value at different time points.

## Autocovariance Function {.smaller}

The autocovariance function of a weakly stationary time series is a **non-negative definite function** over the set of integers. That is, for any positive integer $k$, and any choice of $k$ complex numbers $a_1, \ldots, a_k$, we have

$$
\sum_{i=1}^k \sum_{j=1}^k a_i \bar{a}_j \gamma(|i-j|) \geq 0.
$$

This is the same as saying that for any positive integer $k$, the **symmetric Toeplitz matrix** formed as

$$
\begin{bmatrix}
\gamma(0) & \gamma(1) & \gamma(2) & \cdots & \gamma(k-1) \\
\gamma(-1) & \gamma(0) & \gamma(1) & \cdots & \gamma(k-2) \\
\gamma(-2) & \gamma(-1) & \gamma(0) & \cdots & \gamma(k-3) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\gamma(-k+1) & \gamma(-k + 2) & \gamma(-k + 3) & \cdots & \gamma(0)
\end{bmatrix}
$$

is non-negative definite. In the univariate case, where $C$ is a scalar-valued function, it is symmetric, i.e., $\gamma(-h) = \gamma(h)$ for an integer $h$.

## ACF plots

```{r}
x <- arima.sim(n = 100, model = list(ma = .75))
plot(x)
```

## ACF plots

```{r}
acf(x)
```

::: callout-note
## Beware of spurious significant ACF lags

Many times you will see significant ACF lags that have no practical meaning or implication. You can *usually* ignore these
:::

## Estimating ACF

If a time series is stationary, the mean function (1.22) \$ \mu\_t = \mu \$ is constant so that we can estimate it by the sample mean,

$$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
$$

## Variance of $\bar{x}$ {.smaller}

$$
\begin{align*}
\text{var}(\bar{x}) 
& = 
\text{var} \left( \frac{1}{n}\sum_{t=1}^{n} x_t \right) \\
& = \frac{1}{n^2} \text{cov} \left(\sum_{t=1}^{n}x_t, ~ \sum_{s=1}^{n} x_s \right) \\
& = \frac{1}{n^2} \left( n \gamma_x(0) + (n-1) \gamma_x(1) + (n-2) \gamma_x(2) + \dots + \gamma_x(n-1) \right) \\
& \quad + \frac{1}{n} \left( (n-1) \gamma_x(-1) + (n-2) \gamma_x(-2) + \dots + \gamma_x(1-n) \right) \\
& = \frac{1}{n} \sum_{h=-n}^{n} \left( 1 - \frac{|h|}{n} \right) \gamma_x(h).
\end{align*}
$$

## Sample ACF

The sample autocovariance function is defined as

$$
\widehat{\gamma}(h) = \frac{1}{n} \sum_{t=1}^{n-h} (x_{t+h} - \bar{x})(x_t - \bar{x}),
$$

with $\widehat{\gamma}(-h) = \widehat{\gamma}(h)$ for $h = 0, 1, \ldots, n - 1$.

# ---- Filtering

## Linear Filter

This method is useful in discovering certain traits in a time series, such as long-term trend and seasonal components.

$$
m_t = 
\sum_{j = -k}^{k} a_j x_{t-j}
$$

## Example linear filter

```{r}
wgts = c(.5, rep(1,11), .5)/12
soif = stats::filter(soi, sides=2, filter=wgts)
tsplot(soi, col=4)
lines(soif, lwd=2, col=6)
par(fig = c(.75, 1, .75, 1), new = TRUE) # the insert
nwgts = c(rep(0,20), wgts, rep(0,20))
plot(nwgts, type="l", ylim = c(-.02,.1), xaxt='n', yaxt='n', ann=FALSE)
```

## Henerson Trend filters 

- Henderson moving averages are filters which were derived by Robert Henderson in 1916 for use in actuarial applications.
- They are trend filters, commonly used in time series analysis to smooth seasonally adjusted estimates in order to generate a trend estimate. 
- They are used in preference to simpler moving averages because they can reproduce polynomials of up to degree 3, thereby capturing trend turning points.

```{r}
term_5 <- c(-0.073, 0.294, 0.558, 0.294, -0.073)
term_7 <- c(-0.059, 0.059, 0.294, 0.412, 0.294, 0.059, -0.059)
term_9 <- c(-0.041, -0.010, 0.119, 0.267, 0.330, 0.267, 0.119, -0.010, -0.041)
term_13 <- c(-0.019, -0.028, 0.0, 0.066, 0.147, 0.214, 0.240, 0.214, 0.147, 0.066, 0.0, -0.028, -0.019)
term_23 <- c(-0.004, -0.011, -0.016, -0.015, -0.005, 0.013, 0.039, 0.068, 0.097, 0.122, 0.138, 0.148, 0.138, 0.122, 0.097, 0.068, 0.039, 0.013, -0.005, -0.015, -0.016, -0.011, -0.004)
```

## Henderson Trend filters

```{r, echo=FALSE}
hend = data.frame(time = -11:11,
                  hend_5 = rep(NA, 23),
                  hend_7 = rep(NA, 23),
                  hend_9 = rep(NA, 23),
                  hend_13 = rep(NA, 23),
                  hend_23 = term_23)
hend$hend_5[match(-2:2, -11:11)] <- term_5
hend$hend_7[match(-3:3, -11:11)] <- term_7
hend$hend_9[match(-4:4, -11:11)] <- term_9
hend$hend_13[match(-6:6, -11:11)] <- term_13
hend |>
  gather("hend_filter", "weight", -time) |>
  ggplot(aes(x = time, y = weight, col = hend_filter)) +
  geom_line()
```

## Kernal Smoothing

Kernel smoothing is a moving average smoother that uses a weight function, or kernel, to average the observations.

$$
m_t = \sum_{i=1}^{n} w_i(t) x_i,
$$

$$
w_i(t) 
=
\frac{K\left(\frac{t-i}{b}\right)}{\sum_{j=1}^{n}K\left(\frac{t-j}{b}\right)}
$$

Example, the normal kernal

$$
K(z) = (2\pi)^{-1/2} \exp(-z^2/2)
$$

## Kernal Smoothing Example

```{r}
tsplot(soi, col=4)
lines(ksmooth(time(soi), soi, "normal", bandwidth=1), lwd=2, col=6)
par(fig = c(.75, 1, .75, 1), new = TRUE) # the insert
curve(dnorm, -3, 3,  xaxt='n', yaxt='n', ann=FALSE)
```

## Other filtering/smoothing methods we may discuss

-   LOWESS
-   smoothing splines
-   Smoothing One Series as a Function of Another

# ---- Examples

## Using R `reprex` package to discuss problems

[https://reprex.tidyverse.org]()

1.  Lets say you copy this code onto your clipboard

```{r, eval=FALSE}
(y <- 1:4)
mean(y)
```

1.  Then run `reprex()`
2.  A nicely rendered HTML preview will display in RStudios Viewer
3.  AND copied to your clipboard to be pasted on Github
